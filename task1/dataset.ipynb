{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step to dataset creation, I decided, as it was recommended in the task description, to discover possibilities of using ChatGPT. It appeared to me that such poferful transformer model has to be a perfect generator of a synthetic dataset about mountains as it must be aware of similar texts in internet and its main task is to actually generate new text, based on its knowledge. Furthermore, it has to be aware of how NER works how to build datasets with appropriate structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What I've found out is that under precise instructions, ChatGPT as is, can generate really quality dataset, format it appropriately, and advice on how to actually use it. But the problem was that ChatGPT was really restrained on number of rows it could generate in one turn. When asked for more, it kept proposing scripts, that would combine different parts of sentences to generate any number of rows. Such dataset could be big enough, but not very useful as it would contain a limited number of actual sentence patterns in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script, made by chatGPT, that should create a dataset of 1000 unique sentences about mountains using 100 different patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Whitney</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>was</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>named</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>after</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>legendary</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>figure.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>Mont</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>Blanc's</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>peak</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>is</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>one</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>of</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>the</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>most</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>sought</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>after.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>Tales</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>of</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>Vesuvius</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id     tokens  ner_tags\n",
       "0    0    Whitney         1\n",
       "1    0        was         0\n",
       "2    0      named         0\n",
       "3    0      after         0\n",
       "4    0          a         0\n",
       "5    0  legendary         0\n",
       "6    0    figure.         0\n",
       "7    1       Mont         0\n",
       "8    1    Blanc's         0\n",
       "9    1       peak         0\n",
       "10   1         is         0\n",
       "11   1        one         0\n",
       "12   1         of         0\n",
       "13   1        the         0\n",
       "14   1       most         0\n",
       "15   1     sought         0\n",
       "16   1     after.         0\n",
       "17   2      Tales         0\n",
       "18   2         of         0\n",
       "19   2   Vesuvius         1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Function to generate a very diverse dataset of 1,000 sentences\n",
    "def generate_diverse_ner_data(num_sentences, num_templates, num_mountains):\n",
    "    # Approximately 100 sentence templates\n",
    "    sentence_templates = [\n",
    "        \"{} rises above its surrounding terrain.\", \"{} can be seen from miles away.\",\n",
    "        \"The beauty of {} is unmatched.\", \"{} is on the bucket list of many climbers.\",\n",
    "        \"Tales of {} have inspired adventurers for generations.\", \"The summit of {} was reached in an epic expedition.\",\n",
    "        \"The slopes of {} are treacherous.\", \"{} stands as a beacon for explorers.\",\n",
    "        \"The history of {} is as fascinating as its geography.\", \"Many myths surround the mystical {}.\",\n",
    "        \"The ascent of {} is a formidable challenge.\", \"{}'s peak is one of the most sought after.\",\n",
    "        \"The routes to {} vary from easy to extremely difficult.\", \"Ancient stories tell of gods dwelling on {}.\",\n",
    "        \"{} is part of a larger range of mountains.\", \"The first ascent of {} is still debated.\",\n",
    "        \"The view from the top of {} is breathtaking.\", \"{} was named after a legendary figure.\",\n",
    "        \"The glaciers on {} are slowly melting.\", \"Wildlife around {} is abundant.\",\n",
    "        # ... 80 more templates\n",
    "    ]\n",
    "\n",
    "    # Expand the list to reach approximately 100 unique templates\n",
    "    while len(sentence_templates) < num_templates:\n",
    "        template = random.choice(sentence_templates)\n",
    "        # Add some variation to the template\n",
    "        variations = [\n",
    "            template.replace(\"is\", \"remains\"),\n",
    "            template.replace(\"The\", \"A\"),\n",
    "            template.replace(\"many\", \"various\"),\n",
    "            template.replace(\"miles\", \"kilometers\"),\n",
    "        ]\n",
    "        sentence_templates.extend(variations)\n",
    "        sentence_templates = list(set(sentence_templates))  # Remove duplicates\n",
    "\n",
    "    # Approximately 100 mountain names\n",
    "    mountain_names = [\n",
    "        \"Everest\", \"Kilimanjaro\", \"Denali\", \"McKinley\", \"K2\", \"Annapurna\",\n",
    "        \"Elbrus\", \"Aconcagua\", \"Fuji\", \"Olympus\", \"Mont Blanc\", \"Matterhorn\",\n",
    "        \"Eiger\", \"Vesuvius\", \"Rainier\", \"Whitney\", \"Kangchenjunga\", \"Lhotse\",\n",
    "        \"Makalu\", \"Cho Oyu\",\n",
    "        # ... 80 more names\n",
    "    ]\n",
    "\n",
    "    # Expand the list to reach approximately 100 unique mountain names\n",
    "    while len(mountain_names) < num_mountains:\n",
    "        mountain = random.choice(mountain_names)\n",
    "        # Add some variation to the mountain name\n",
    "        variations = [\n",
    "            mountain + \" Peak\",\n",
    "            \"Mount \" + mountain,\n",
    "            mountain + \" Range\",\n",
    "            \"The Great \" + mountain\n",
    "        ]\n",
    "        mountain_names.extend(variations)\n",
    "        mountain_names = list(set(mountain_names))  # Remove duplicates\n",
    "\n",
    "    # Generate the sentences\n",
    "    sentences = []\n",
    "    for _ in range(num_sentences):\n",
    "        template = random.choice(sentence_templates)\n",
    "        mountain = random.choice(mountain_names)\n",
    "        sentence = template.format(mountain)\n",
    "        sentences.append(sentence)\n",
    "\n",
    "    # Shuffle the sentences to add randomness\n",
    "    random.shuffle(sentences)\n",
    "\n",
    "    # Convert to DataFrame for NER tagging\n",
    "    ner_data = []\n",
    "    for sentence_id, sentence in enumerate(sentences):\n",
    "        tokens = sentence.split()\n",
    "        for token in tokens:\n",
    "            # Randomly decide if a token is a mountain name\n",
    "            if token in mountain_names:\n",
    "                # Randomly assign B-MNT or I-MNT tags to mountain names\n",
    "                tag = np.random.choice([1, 2], p=[0.7, 0.3])\n",
    "            else:\n",
    "                tag = 0\n",
    "            ner_data.append([sentence_id, token, tag])\n",
    "    \n",
    "    return pd.DataFrame(ner_data, columns=[\"id\", \"tokens\", \"ner_tags\"])\n",
    "\n",
    "# Generate 1,000 diverse sentences\n",
    "# diverse_ner_data = generate_diverse_ner_data(1000, 100, 100)\n",
    "diverse_ner_data = generate_diverse_ner_data(100, 10, 10)\n",
    "\n",
    "# Display the first few rows of the generated dataset\n",
    "diverse_ner_data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, I've tried using Premium version of ChatGPT and it actually started to generate csv files, each 1000 sentences in appropriate format, ready to use. Here is how it looked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nid,tokens,ner_tags\\n0,Mount,1\\n0,Vesuvius,2\\n0,is,0\\n0,known,0\\n0,for,0\\n0,its,0\\n0,breathtaking,0\\n0,views,0\\n0,and,0\\n0,challenging,0\\n0,trails,0\\n1,Mount,1\\n1,Vesuvius,2\\n1,Mount,0\\n1,is,0\\n1,the,0\\n1,highest,0\\n1,peak,0\\n1,in,0\\n1,the,0\\n1,region,0\\n1,of,0\\n2,Mount,1\\n2,Matterhorn,2\\n2,has,0\\n2,a,0\\n2,rich,0\\n2,history,0\\n2,and,0\\n2,is,0\\n2,a,0\\n2,symbol,0\\n2,of,0\\n3,Mount,1\\n3,Matterhorn,2\\n3,is,0\\n3,located,0\\n3,in,0\\n3,the,0\\n3,beautiful,0\\n3,landscape,0\\n3,of,0\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "I'd like you to generate a synthetic dataset for me. I need to use it for finetuning of bert model for NER of mountains so it should be in appropriate format.\n",
    "I need 3 rows and columns id, tokens, ner_tags. Use 3 tags: 0, 1, 2 that correspond to O, B-MNT, I-MNT. Give me 100 rows for a start.\n",
    "Here is a short example of desired output: ...\n",
    "'''\n",
    "\n",
    "'''\n",
    "id,tokens,ner_tags\n",
    "0,Mount,1\n",
    "0,Vesuvius,2\n",
    "0,is,0\n",
    "0,known,0\n",
    "0,for,0\n",
    "0,its,0\n",
    "0,breathtaking,0\n",
    "0,views,0\n",
    "0,and,0\n",
    "0,challenging,0\n",
    "0,trails,0\n",
    "1,Mount,1\n",
    "1,Vesuvius,2\n",
    "1,Mount,0\n",
    "1,is,0\n",
    "1,the,0\n",
    "1,highest,0\n",
    "1,peak,0\n",
    "1,in,0\n",
    "1,the,0\n",
    "1,region,0\n",
    "1,of,0\n",
    "2,Mount,1\n",
    "2,Matterhorn,2\n",
    "2,has,0\n",
    "2,a,0\n",
    "2,rich,0\n",
    "2,history,0\n",
    "2,and,0\n",
    "2,is,0\n",
    "2,a,0\n",
    "2,symbol,0\n",
    "2,of,0\n",
    "3,Mount,1\n",
    "3,Matterhorn,2\n",
    "3,is,0\n",
    "3,located,0\n",
    "3,in,0\n",
    "3,the,0\n",
    "3,beautiful,0\n",
    "3,landscape,0\n",
    "3,of,0\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentences appeared to be too short and not enough diverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nid,tokens,ner_tags\\n0,Mount,1\\n0,Fuji,2\\n0,offers,0\\n0,some,0\\n0,of,0\\n0,the,0\\n0,most,0\\n0,spectacular,0\\n0,hiking,0\\n0,routes,0\\n0,in,0\\n1,Mount,1\\n1,McKinley,2\\n1,Mount,0\\n1,is,0\\n1,the,0\\n1,highest,0\\n1,peak,0\\n1,in,0\\n1,the,0\\n1,region,0\\n1,of,0\\n2,Mount,1\\n2,Rainier,2\\n2,Mount,0\\n2,is,0\\n2,the,0\\n2,highest,0\\n2,peak,0\\n2,in,0\\n2,the,0\\n2,region,0\\n2,of,0\\n3,Mount,1\\n3,Vesuvius,2\\n3,is,0\\n3,famous,0\\n3,for,0\\n3,its,0\\n3,snow-capped,0\\n3,peaks,0\\n3,and,0\\n3,alpine,0\\n3,flora,0\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Increse the sentence lengths to mean of 7 words and make them more different. Try to partially use public data and real sentences for generation. \n",
    "Also try to write about mountains from different locations\n",
    "'''\n",
    "\n",
    "'''\n",
    "id,tokens,ner_tags\n",
    "0,Mount,1\n",
    "0,Fuji,2\n",
    "0,offers,0\n",
    "0,some,0\n",
    "0,of,0\n",
    "0,the,0\n",
    "0,most,0\n",
    "0,spectacular,0\n",
    "0,hiking,0\n",
    "0,routes,0\n",
    "0,in,0\n",
    "1,Mount,1\n",
    "1,McKinley,2\n",
    "1,Mount,0\n",
    "1,is,0\n",
    "1,the,0\n",
    "1,highest,0\n",
    "1,peak,0\n",
    "1,in,0\n",
    "1,the,0\n",
    "1,region,0\n",
    "1,of,0\n",
    "2,Mount,1\n",
    "2,Rainier,2\n",
    "2,Mount,0\n",
    "2,is,0\n",
    "2,the,0\n",
    "2,highest,0\n",
    "2,peak,0\n",
    "2,in,0\n",
    "2,the,0\n",
    "2,region,0\n",
    "2,of,0\n",
    "3,Mount,1\n",
    "3,Vesuvius,2\n",
    "3,is,0\n",
    "3,famous,0\n",
    "3,for,0\n",
    "3,its,0\n",
    "3,snow-capped,0\n",
    "3,peaks,0\n",
    "3,and,0\n",
    "3,alpine,0\n",
    "3,flora,0\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was better, but whem I asked for 5000 sentences, it silentlyh generated only 959. So I decided that I've found the limit and started to experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nid,tokens,ner_tags\\n0,Many,0\\n0,climbers,0\\n0,dream,0\\n0,of,0\\n0,scaling,0\\n0,the,0\\n0,summit,0\\n0,of,0\\n0,a,0\\n0,great,0\\n0,peak,0\\n1,The,0\\n1,expedition,0\\n1,team,0\\n1,set,0\\n1,up,0\\n1,camp,0\\n1,at,0\\n1,the,0\\n1,base,0\\n1,of,0\\n2,The,0\\n2,expedition,0\\n2,team,0\\n2,set,0\\n2,up,0\\n2,camp,0\\n2,at,0\\n2,the,0\\n2,base,0\\n2,of,0\\n3,The,0\\n3,legend,0\\n3,of,0\\n3,revolves,0\\n3,around,0\\n3,the,0\\n3,mythical,0\\n3,creatures,0\\n3,of,0\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Make another 1k rows but add some tricky case with present and abcent mountains\n",
    "'''\n",
    "\n",
    "'''\n",
    "id,tokens,ner_tags\n",
    "0,Many,0\n",
    "0,climbers,0\n",
    "0,dream,0\n",
    "0,of,0\n",
    "0,scaling,0\n",
    "0,the,0\n",
    "0,summit,0\n",
    "0,of,0\n",
    "0,a,0\n",
    "0,great,0\n",
    "0,peak,0\n",
    "1,The,0\n",
    "1,expedition,0\n",
    "1,team,0\n",
    "1,set,0\n",
    "1,up,0\n",
    "1,camp,0\n",
    "1,at,0\n",
    "1,the,0\n",
    "1,base,0\n",
    "1,of,0\n",
    "2,The,0\n",
    "2,expedition,0\n",
    "2,team,0\n",
    "2,set,0\n",
    "2,up,0\n",
    "2,camp,0\n",
    "2,at,0\n",
    "2,the,0\n",
    "2,base,0\n",
    "2,of,0\n",
    "3,The,0\n",
    "3,legend,0\n",
    "3,of,0\n",
    "3,revolves,0\n",
    "3,around,0\n",
    "3,the,0\n",
    "3,mythical,0\n",
    "3,creatures,0\n",
    "3,of,0\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point I discovered, that the sentences were not actually all unique in the dataset, and after all tries it turned out that, ChatGPT, premium or not, can only generate 50 really unique sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIn responce to this, ChatGPT actually wrote a python script for dataset generation, and broke\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Make another 1000 sentences, but make it very very diverse. Use about 50 different sentence templates and about 50 mountains and combine them into 1000 sentences.\n",
    "Add some randomness to it. The result should look like it was scraped over the internet, not generated by a short and determined script.\n",
    "Be super creative about it. And stop putting Mount before each mountain, this it too obvious.\n",
    "'''\n",
    "\n",
    "'''\n",
    "In responce to this, ChatGPT actually wrote a python script for dataset generation, and broke\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nid,tokens,ner_tags\\n0,The,0\\n0,first,0\\n0,ascent,0\\n0,of,0\\n0,K2,1\\n0,was,0\\n0,a,0\\n0,monumental,0\\n0,event,0\\n0,in,0\\n0,mountaineering,0\\n0,history.,0\\n1,The,0\\n1,route,0\\n1,to,0\\n1,Kangchenjunga,1\\n1,is,0\\n1,dotted,0\\n1,with,0\\n1,both,0\\n1,beauty,0\\n1,and,0\\n1,peril.,0\\n2,K2,1\\n2,is,0\\n2,often,0\\n2,shrouded,0\\n2,in,0\\n2,\"mist,\",0\\n2,adding,0\\n2,to,0\\n2,its,0\\n2,mystique.,0\\n3,Annapurna,1\\n3,stands,0\\n3,as,0\\n3,an,0\\n3,icon,0\\n3,in,0\\n3,the,0\\n3,Fuji,1\\n3,landscape.,0\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Make another 1000 sentences but use AI generation instead of plain scripts. The result should not have so many repeats of identical sentence parts.\n",
    "'''\n",
    "\n",
    "'''\n",
    "id,tokens,ner_tags\n",
    "0,The,0\n",
    "0,first,0\n",
    "0,ascent,0\n",
    "0,of,0\n",
    "0,K2,1\n",
    "0,was,0\n",
    "0,a,0\n",
    "0,monumental,0\n",
    "0,event,0\n",
    "0,in,0\n",
    "0,mountaineering,0\n",
    "0,history.,0\n",
    "1,The,0\n",
    "1,route,0\n",
    "1,to,0\n",
    "1,Kangchenjunga,1\n",
    "1,is,0\n",
    "1,dotted,0\n",
    "1,with,0\n",
    "1,both,0\n",
    "1,beauty,0\n",
    "1,and,0\n",
    "1,peril.,0\n",
    "2,K2,1\n",
    "2,is,0\n",
    "2,often,0\n",
    "2,shrouded,0\n",
    "2,in,0\n",
    "2,\"mist,\",0\n",
    "2,adding,0\n",
    "2,to,0\n",
    "2,its,0\n",
    "2,mystique.,0\n",
    "3,Annapurna,1\n",
    "3,stands,0\n",
    "3,as,0\n",
    "3,an,0\n",
    "3,icon,0\n",
    "3,in,0\n",
    "3,the,0\n",
    "3,Fuji,1\n",
    "3,landscape.,0\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, I decided on using combined dataset of all previously generated chunks, as at least they differ from each other, and generate some more in small batches with other adjustments e.g. without word Mount in each sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But before that, I just wanted to check another opportunity with Open API. The idea was to create a pool of different variations of prompts asking for unique dataset about mountains for NER and iteratively create a big dataset from small chunks that Open API would return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "client = OpenAI(\n",
    "    os.environ.get(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "def infer_gpt(prompt):\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,\n",
    "                }\n",
    "            ],\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            temperature=0.6,\n",
    "            max_tokens=3500,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0.02,\n",
    "            presence_penalty=0.2,\n",
    "            timeout=120,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print('Exception: ', e)\n",
    "        return []\n",
    "    response = json.loads(resp)['choises'][0]['text']\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, my Open AI quota was already used up, so I decided to leave this opportunity for improvements chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUMMARY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've discovered several ways to build a dataset, including the basic ones, that weren't described yet:\n",
    "- Internet scraping\n",
    "- prompting ChatGPT for generator script\n",
    "- prompting ChatGPT for a chunk with unique sentences\n",
    "- using openai to iteratively build dataset from chunks\n",
    "\n",
    "The best way would be to combine all these ideas and make a dataset that would contain scraped text and build on them with openapi and diverse prompts for datset chunks.\n",
    "\n",
    "One important note is that training and validation sets shoud be built independently, so the pattern used do not overlap significantly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
